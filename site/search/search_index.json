{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Homepage","text":"css add shadow <p>Hi, this is Qihang I\u2019m documenting my learning process in this notes. I built this project with Mkdocs and use the theme of Material for MkDocs. It's a really powerful tool to build blog or notes.</p> <p>I will post some notes on important courses, interesting papers, as well as how to use technical tools such as Git and Overleaf. Maybe there will be much more interesting content in the future. If you are also interested in those topics, feel free to contact me, and I am really happy to discuss with various people.</p> <p>Here is the instruction on how to build a website with Material for MkDocs: How to build your personal website/blog with MkDocs (Start from Material for MkDocs)</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html","title":"EECE 566 Part1 Concept Summary","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#source-and-channel-concepts","title":"Source And Channel Concepts","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#dms","title":"DMS","text":"<p>DMS: Discrete Memoryless Source</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#bss-in-dms","title":"BSS \\(\\in\\) DMS","text":"<p>BSS: Binary Symmetric Source</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#dmc","title":"DMC","text":"<p>DMC: Discrete Memoryless Channel</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#bsc","title":"BSC","text":"<p>BSC: Binary Symmetric Channel</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#bec","title":"BEC","text":"<p>BEC: Binary Erasure Channel</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#ber","title":"BER","text":"<p>BER: Bit Error Rate </p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#bpsk","title":"BPSK","text":"<p>BPSK: Binary Phase Shift Keying -- a special case of binary antipodal signaling</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#awgnchannel","title":"AWGN(channel)","text":"<p>AWGN: Additive White Gaussian Noise </p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#agnchannel","title":"AGN(channel)","text":"<p>AGN: Additive Gaussian Noise</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#coding-gain","title":"Coding Gain","text":"<p>Coding Gain: The difference between the Energy required to achieve a given BER with coding and the Energy required to achieve the same BER without coding.</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#hamming-distance","title":"Hamming Distance","text":"<p>Hamming Distance: The number of bit positions in which two code words differ.</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#channel-capacity","title":"Channel Capacity","text":"<p>Channel Capacity: The maximum rate at which information can be transmitted over a given communication channel in the presence of noise. $$     C = \\max_{p(x)} I(X;Y) = \\max_{p(x)} H(Y) - H(Y|X) = \\max_{p(x)} H(X) - H(X|Y)\\ $$</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#probability","title":"Probability","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#gussian-pdf","title":"Gussian pdf:","text":"\\[     f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#gussian-cdf","title":"Gussian cdf:","text":"\\[     F(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\int_{-\\infty}^{x}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#gussian-generating-function","title":"Gussian Generating Function:","text":"\\[     \\phi(s)=E[e^{sX}]=e^{\\mu s+\\frac{1}{2}\\sigma^2s^2}\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#qalpha","title":"\\(Q(\\alpha)\\)","text":"<p>Q function is the tail probability of the standard normal distribution. It is defined as: $$     Q(\\alpha)=\\frac{1}{\\sqrt{2\\pi}}\\int_{\\alpha}^{\\infty}e^{-\\frac{x^2}{2}}dx\\ $$</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#measure-of-information","title":"Measure of Information","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#self-entropy","title":"Self Entropy","text":"\\[     \\mathrm {H} (X)=-\\sum _{x\\in {\\mathcal {X}}}\\,p(x)\\,\\log \\,p(x)\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#binary-entropy-function","title":"Binary Entropy Function","text":"\\[ \\mathrm {H} (X)=-\\sum _{x\\in {\\mathcal {X}}}\\,p(x)\\,\\log\\,p(x)\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#joint-entropy","title":"Joint Entropy","text":"\\[     \\mathrm {H} (X,Y)=-\\sum _{x\\in {\\mathcal {X}},y\\in {\\mathcal {Y}}}\\,p(x,y)\\,\\log \\,p(x,y)\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#properties","title":"Properties","text":"\\[     \\mathrm {H} (X,Y)\\leq \\mathrm {H} (X)+\\mathrm {H} (Y) \\, iff \\, X \\, and \\, Y \\, are \\, independent\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#conditional-entropy","title":"Conditional Entropy","text":"\\[     \\mathrm {H} (Y|X)=-\\sum _{x\\in {\\mathcal {X}},y\\in {\\mathcal {Y}}}\\,p(x,y)\\,\\log \\,p(y|x)\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#properties_1","title":"Properties","text":"\\[     \\mathrm {H} (Y|X)\\leq \\mathrm {H} (Y)\\\\ \\] \\[ \\mathrm {H} (Y|X) = \\sum _{x\\in {\\mathcal {X}}}\\,p(x)\\,\\mathrm {H} (Y|X=x) \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#relative-entropy","title":"Relative Entropy","text":"<p>(KL Divergence)</p> \\[     \\mathrm {D} _{\\mathrm {KL} }(P\\parallel Q)=\\sum _{x\\in {\\mathcal {X}}}\\,p(x)\\,\\log {\\frac {p(x)}{q(x)}}\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#properties_2","title":"Properties","text":"<p>positive semi-definite:</p> \\[     \\mathrm {D} _{\\mathrm {KL} }(P\\parallel Q)\\geq 0\\\\  \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#mutual-information","title":"Mutual Information","text":"\\[     \\mathrm {I} (X;Y)=\\sum _{x\\in {\\mathcal {X}},y\\in {\\mathcal {Y}}}\\,p(x,y)\\,\\log {\\frac {p(x,y)}{p(x)\\,p(y)}}\\\\ \\] <p>Relation ship between KL Divergence and Mutual Information:</p> \\[     \\mathrm {I} (X;Y)=\\mathrm {D} _{\\mathrm {KL} }(P(X,Y)\\parallel P(X)P(Y))\\\\   \\] <p>Conditional Mutual Information     $$          \\mathrm {I} (X;Y|Z)=\\mathrm {H} (X|Z)-\\mathrm {H} (X|Y,Z)\\      $$</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#relationship-between-hxy-hxy-and-hyx","title":"\\(Relationship \\,between \\,H(X,Y), H(X|Y), and \\, H(Y|X)\\)","text":"<p>Relation ship between Joint Entropy and Conditional Entropy: $$         \\mathrm {H} (X,Y)=\\mathrm {H} (X)+\\mathrm {H} (Y|X)\\ $$ Relation ship between Mutual Information and Conditional Entropy: $$         \\mathrm {I} (X;Y)=\\mathrm {H} (X)-\\mathrm {H} (X|Y)\\ $$ Relation ship between Mutual Information and Joint Entropy: $$         \\mathrm{H}(X) - \\mathrm{I}(X;Y) = \\mathrm{H} (X|Y) = \\mathrm{H} (X,Y) - \\mathrm{H} (Y)\\ $$ $$ \\mathrm {I} (X;Y)=\\mathrm {H} (X)+\\mathrm {H} (Y)-\\mathrm {H} (X,Y)\\ $$</p> <p></p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#source-coding","title":"Source Coding","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#uniquely-decodableud","title":"Uniquely decodable(U.D.)","text":"<p>Uniquely decodable(U.D.): A code is uniquely decodable if different source sequences are mapped into different cw sequences.</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#prefix-freeprefix","title":"Prefix-free/Prefix","text":"<p>Instantaneous code/Prefix-free: A code is prefix-free if no cw is a prefix of any other cw.</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#theorem-kraft-inequality","title":"Theorem Kraft Inequality","text":"<p>For prefix-free codes, the following inequality must be satisfied: D: The number of symbols in the alphabet.</p> <p>M: The number of codewords.</p> <p>\\(n_i\\): The length of the i-th codeword.</p> \\[     \\sum_{i=1}^{M}D^{-n_i}\\leq 1\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#thereom-mcmillan-inequality","title":"Thereom McMillan Inequality","text":"<p>For uniquely decodable codes, the following inequality must be satisfied: $$     \\sum_{i=1}^{M}D^{-n_i}\\leq 1\\  $$</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#variable-length-source-coding-theorem","title":"Variable-length source coding theorem","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#average-codeword-length","title":"average codeword length","text":"\\[     \\bar{N}=\\sum_{i=1}^{M}p_i n_i\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#entropy","title":"Entropy","text":"\\[     H_D(X)=-\\sum_{i=1}^{M}p_i\\log_D p_i\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#lower-bound","title":"Lower bound","text":"\\[     \\bar{N}\\geq H_D(X)\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#upper-bound","title":"Upper bound","text":"\\[     \\bar{N} &lt; H_D(X)+1\\\\ \\] <p>Choose \\(n_i = \\lceil \\log_D \\frac{1}{p_i} \\rceil\\) to minimize \\(\\bar{N}\\), then \\(\\bar{N} = H_D(X) + 1\\).</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#huffman-coding","title":"Huffman Coding","text":"<p>(Optimal variable-length source coding)</p> <p>Skip its Proof</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#fixed-length-source-coding","title":"Fixed-length source coding","text":"<p>A sequence of L consecutive symbols from a DMS of alphabet size M $$     U_L = \\{ U_1,U_2,...,U_L\\} $$ Suppose weuse a code alphabet of size D to encode these sequences into fixed length codewords of length N.</p> \\[     D^N\\geq M^L\\\\ \\] \\[     \\frac{N}{L} \\geq \\frac{\\log M}{\\log D}\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#universal-source-coding","title":"Universal Source Coding","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#lempel-ziv-algorithm","title":"Lempel-Ziv Algorithm","text":"<p>Skip</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#channel-coding","title":"Channel Coding","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#dmc_1","title":"DMC","text":"<p>DMC: Discrete Memoryless Channel</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#shannons-capacity-of-a-dmc","title":"Shannon's capacity of a DMC","text":"\\[     C = \\max_{p(x)} I(X;Y) \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#calculation","title":"Calculation","text":"\\[     C = \\max_{p(x)} I(X;Y) = \\max_{p(x)} H(Y) - H(Y|X) = \\max_{p(x)} H(X) - H(X|Y)\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#symmetric-dmc","title":"Symmetric DMC","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#definition","title":"Definition","text":"<p>A DMC is symmetric if the output symbols can be  partitioned into subsets in such a way that for each subset each row is a permutation of the other row and each column is a permutation of the other column.</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#properties_3","title":"Properties","text":"<p>For a symmetric DMC, the capacity is achieved by a uniform input distribution.</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#channel-capacity-theorem","title":"Channel Capacity Theorem","text":"<p> \\(p_{e,l} = Pr \\\\{ \\hat{U_l} \\not= U_l \\\\}\\)</p> <p>\\(P_e = \\frac{1}{L} \\sum_{l=1}^{L} p_{e,l}\\) $$ P_elog(M - 1) + h(P_e) \\geq H(U) - \\frac{\\tau_s}{\\tau_c}C\\ $$</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#equality-holds-when","title":"equality holds when","text":"<p>1) \\(U_l = X_n\\), \\(Y_n = \\hat{U_l}\\)</p> <p>2) \\(\\hat{U_l}\\) and \\(\\mathrm{1}\\{U_l \\not= \\hat{u_l}\\}\\) are independent</p>"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#markov-inequality","title":"Markov Inequality","text":"\\[     Pr \\{ X \\geq \\sigma \\} \\leq \\frac{E[X]}{\\sigma}\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#chernoff-bound","title":"Chernoff Bound","text":"\\[     Pr \\{ X \\geq \\sigma \\} \\leq \\frac{E[e^{sX}]}{e^{s\\sigma}}, \\, s &gt; 0\\\\ \\] \\[     Pr \\{ X \\geq \\sigma \\} \\geq \\frac{E[e^{sX}]}{e^{s\\sigma}}, \\, s &lt; 0\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#continuous-random-variables","title":"Continuous random variables","text":""},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#entropy_1","title":"Entropy","text":"\\[     H(X) = -\\int_{-\\infty}^{\\infty}f(x)\\log f(x)dx\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#entropy-of-gaussian-random-variable","title":"Entropy of Gaussian random variable","text":"\\[     H(X) = \\frac{1}{2}\\log(2\\pi e\\sigma^2)\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#mutual-information_1","title":"Mutual Information","text":"\\[     I(X;Y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f(x,y)\\log\\frac{f(x,y)}{f(x)f(y)}dxdy\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#capacity-of-gaussian-channel","title":"Capacity of Gaussian channel","text":"\\[     C = \\frac{1}{2}\\log(1+\\frac{E}{\\sigma^2})\\\\ \\]"},{"location":"Courses/EECE%20566/Midterm/Concept%20Summary.html#capacity-of-parallel","title":"Capacity of parallel","text":"\\[     C = \\sum_{i=1}^{n}C_i = \\sum_{i=1}^{n}\\frac{1}{2}\\log(1+\\frac{E_i}{\\sigma_i^2})\\\\ \\]"},{"location":"Developing%20Tools/Library/Git/Git-All-In-One.html","title":"Git usage","text":""},{"location":"Developing%20Tools/Library/Git/Git-All-In-One.html#remote","title":"Remote","text":""},{"location":"Developing%20Tools/Library/Git/Git-All-In-One.html#add-remote","title":"Add remote","text":"<pre><code>git remote add origin\n</code></pre>"},{"location":"Developing%20Tools/Library/Git/Git-All-In-One.html#remove-remote","title":"Remove remote","text":"<pre><code>git remote remove origin\n</code></pre>"},{"location":"Developing%20Tools/Library/Git/Git-All-In-One.html#rename-remote","title":"Rename remote","text":"<pre><code>git remote rename origin new_origin\n</code></pre>"},{"location":"Developing%20Tools/Library/Git/Git-All-In-One.html#show-remote","title":"Show remote","text":"<pre><code>git remote -v\n</code></pre>"},{"location":"Developing%20Tools/Library/Git/Git-All-In-One.html#change-remote-url","title":"Change remote url","text":"<pre><code>git remote set-url remote-name remote-url\n</code></pre>"},{"location":"Paper%20Reading/index.html","title":"Blog","text":""},{"location":"Paper%20Reading/2023/10/21/LLM-Tokenizer.html","title":"NLP-Tokenizer","text":"<p>This blog is about the tokenizer in NLP. The tokenizer is the first step in NLP. It is the process of converting a sequence of characters into a sequence of tokens.</p>"},{"location":"Paper%20Reading/2023/10/21/LLM-Tokenizer.html#bpe-origin-byte-pair-encoding","title":"BPE origin (Byte Pair Encoding)","text":""},{"location":"Paper%20Reading/archive/2023.html","title":"2023","text":""},{"location":"Paper%20Reading/category/something.html","title":"Something","text":""}]}